{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Average Modeling\n",
    "\n",
    "Here I want to explore the concept of using weighted averaging for estimation, with multiple base-estimators. In my experience, subject matter experts often crowd-source multiple estimates, sometimes picking the estimate that provides the most comfort, and often averaging predictions together with a weighting scheme. How these weights might be chosen is often arbitrary, or not learned from the data. Here I want to create a linear regression that estimates coefficients as appropriate weights over several base estimators. In order to do that, let's create a few (3) base estimators for a regression problem, look at their test errors, and then build a regression with those base estimator predictions with conditions to ensure that the resulting model is interpretable as a weighted-average calculation:\n",
    "\n",
    "1. $ \\forall b \\in B, \\ 0 \\leq b \\leq 1 $\n",
    "2. $ b_{norm} = \\frac{b}{\\sum B}, \\ \\forall b \\in B $\n",
    "2. $ \\hat{y} = X \\cdot B_{norm} $\n",
    "\n",
    "All coefficients are bounded between 0 and 1, and each coefficient is normalized by the sum of all coefficients so that they sum to 1, prior to linear estimation.\n",
    "\n",
    "A mathematical expression of the optimization:\n",
    "\n",
    "$ \\hat{y} = X \\cdot B_{norm} $ \n",
    "   \n",
    "$ sse = .5*(\\hat{y} - y)^{2} = .5*(X \\cdot \\begin{bmatrix} b_1*(\\sum{B})^{-1} \\\\ \\ldots \\\\ b_k*(\\sum{B})^{-1} \\end{bmatrix} - y)^{2} $  \n",
    "  \n",
    "$ \\delta_{sse,b_i} = (X \\cdot \\begin{bmatrix} b_1*(\\sum{B})^{-1} \\\\ \\ldots \\\\ b_k*(\\sum{B})^{-1} \\end{bmatrix} - y) \\cdot (x_i * b_i * (-1) * (\\sum{B})^{-2} + x_i * (\\sum{B})^{-1} + \\sum_{\\substack{k \\neq i}}{x_k * b_k * (-1) * (\\sum{B})^{-2}}) $  \n",
    "  \n",
    "$      = (X \\cdot \\begin{bmatrix} b_1*(\\sum{B})^{-1} \\\\ \\ldots \\\\ b_k*(\\sum{B})^{-1} \\end{bmatrix} - y) \\cdot ( X \\cdot \\begin{bmatrix} b_1 * (-1) * (\\sum{B})^{-2} \\\\ \\ldots \\\\ b_k * (-1) * (\\sum{B})^{-2} \\end{bmatrix} + x_i * (\\sum{B})^{-1}) $  \n",
    "  \n",
    "$      = (X \\cdot B * (\\sum{B})^{-1} - y) \\cdot ( X \\cdot B  * (-(\\sum{B})^{-2}) + x_i * (\\sum{B})^{-1}) $\n",
    "\n",
    "Partial derivatives for all coefficients in $ B $ would be expressed similarly. \n",
    "\n",
    "The data I'll use for this exercise is:  \n",
    "Liver Disorders  \n",
    "Donated on 5/14/1990  \n",
    "BUPA Medical Research Ltd. database donated by Richard S. Forsyth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = fetch_openml(data_id=8,as_frame=True,return_X_y=True, parser='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'll create a Linear Regression, Random Forest Regressor (with growth restrictions), and a KNN Regressor, which are algorithms with very different properties, namely linearity, decision-based non-linearity, and spatial considerations. I'll apply the same scaling for preprocessing for all three pipelines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAEs\n",
      "\n",
      "Training:\n",
      " \n",
      "Linear Regression: 2.276 \n",
      "RF: 1.99 \n",
      "KNN: 2.03 \n",
      "\n",
      "Testing:\n",
      " \n",
      "Linear Regression: 2.654 \n",
      "RF: 2.482 \n",
      "KNN: 2.678\n"
     ]
    }
   ],
   "source": [
    "l1 = LinearRegression()\n",
    "l2 = RandomForestRegressor(random_state=42,max_depth=4,n_estimators=50)\n",
    "l3 = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "m1 = Pipeline(\n",
    "    steps=[\n",
    "        ('scaler',MaxAbsScaler()),\n",
    "        ('learner',l1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "m2 = Pipeline(\n",
    "    steps=[\n",
    "        ('scaler',MaxAbsScaler()),\n",
    "        ('learner',l2)\n",
    "    ]\n",
    ")\n",
    "\n",
    "m3 = Pipeline(\n",
    "    steps=[\n",
    "        ('scaler',MaxAbsScaler()),\n",
    "        ('learner',l3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size=.2,random_state=42)\n",
    "\n",
    "m1.fit(Xtrain,ytrain)\n",
    "m2.fit(Xtrain,ytrain)\n",
    "m3.fit(Xtrain,ytrain)\n",
    "\n",
    "p1 = m1.predict(Xtrain)\n",
    "p2 = m2.predict(Xtrain)\n",
    "p3 = m3.predict(Xtrain)\n",
    "\n",
    "t1 = m1.predict(Xtest)\n",
    "t2 = m2.predict(Xtest)\n",
    "t3 = m3.predict(Xtest)\n",
    "\n",
    "mae1 = mean_absolute_error(ytrain,p1)\n",
    "mae2 = mean_absolute_error(ytrain,p2)\n",
    "mae3 = mean_absolute_error(ytrain,p3)\n",
    "\n",
    "tmae1 = mean_absolute_error(ytest,t1)\n",
    "tmae2 = mean_absolute_error(ytest,t2)\n",
    "tmae3 = mean_absolute_error(ytest,t3)\n",
    "\n",
    "print('MAEs\\n')\n",
    "print(\n",
    "    'Training:\\n',\n",
    "    '\\nLinear Regression:',round(mae1,3),\n",
    "    '\\nRF:',round(mae2,3),\n",
    "    '\\nKNN:',round(mae3,3),'\\n')\n",
    "print(\n",
    "    'Testing:\\n',\n",
    "    '\\nLinear Regression:',round(tmae1,3),\n",
    "    '\\nRF:',round(tmae2,3),\n",
    "    '\\nKNN:',round(tmae3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAEs here are with a similar range, and naturally the Random Forest could have been allowed to more tightly fit the training data, and potentially test more strongly. Here I prefer to work with three OK estimators. \n",
    "\n",
    "Now, I'll take the training-set predictions of each, and build the above-mentioned weighted-averaging linear model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned weights:  [0.    0.864 0.136]\n",
      "Training MAE: 1.977\n",
      "Testing MAE: 2.484\n"
     ]
    }
   ],
   "source": [
    "X1 = pd.DataFrame({\n",
    "    'p1':p1,\n",
    "    'p2':p2,\n",
    "    'p3':p3\n",
    "},index=Xtrain.index)\n",
    "n_feats = X1.shape[1]\n",
    "\n",
    "params = np.array([1/n_feats for n in range(n_feats)]) # Initialized with equal weights\n",
    "params = np.clip(params,a_min=0,a_max=1) # Clipped parameter update, so parameters will be bounded from 0 to 1.\n",
    "params = params/sum(params)\n",
    "l = .001\n",
    "\n",
    "for e in range(50):\n",
    "\n",
    "    yhat = X1@params\n",
    "\n",
    "    #sse = np.array((yhat - ytrain)**2).sum() # The SSE, but we don't need this for optimization.\n",
    "\n",
    "    grad = np.array(yhat - ytrain).T@(np.array((X1@params)*(-sum(params)**(-2)))[:, np.newaxis] + np.array(X1*(sum(params)**(-1)))) # The gradient for all parameters.\n",
    "\n",
    "    params = np.clip(params - l*grad,0,1) # Clipped parameter update, so parameters will be bounded from 0 to 1.\n",
    "\n",
    "    params = params/sum(params)\n",
    "    #print(sse)\n",
    "\n",
    "yhat = X1@params/sum(params)\n",
    "\n",
    "mae_ = round(mean_absolute_error(ytrain,yhat),3)\n",
    "\n",
    "X1_test = pd.DataFrame({\n",
    "    'p1':t1,\n",
    "    'p2':t2,\n",
    "    'p3':t3\n",
    "})\n",
    "\n",
    "ypred = X1_test@(params/sum(params))\n",
    "\n",
    "tmae_ = round(mean_absolute_error(ytest,ypred),3)\n",
    "\n",
    "print(\"Learned weights: \",np.round(params/sum(params),3))\n",
    "\n",
    "print(\"Training MAE:\",mae_)\n",
    "print(\"Testing MAE:\",tmae_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the testing MAE is comparable to testing MAE of the Random Forest estimate on its own, and the Random Forest estimator's weight is indeed the dominant weight following optimization, as expected.\n",
    "\n",
    "Interestingly and alternatively, what if a simple linear regression with a positive constraint was applied, without the normalization condition that parameters must sum to 1? The outcome is similar, with the Random Forest weight dominating, after normalizing the resulting coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.85115733, 0.14884267])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = pd.DataFrame({\n",
    "    'p1':p1,\n",
    "    'p2':p2,\n",
    "    'p3':p3\n",
    "})\n",
    "\n",
    "tmp_model = LinearRegression(fit_intercept=False,positive=True).fit(X1,ytrain)\n",
    "\n",
    "tmp_model.coef_/sum(tmp_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the full exercise, but with default Random Forest parameters, allowing that estimate to more closely fit the training data. The expectation here is that the Random Forest estimate would even more strongly dominate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAEs\n",
      "\n",
      "Training:\n",
      " \n",
      "Linear Regression: 2.276 \n",
      "RF: 0.94 \n",
      "KNN: 2.03 \n",
      "\n",
      "Testing:\n",
      " \n",
      "Linear Regression: 2.654 \n",
      "RF: 2.49 \n",
      "KNN: 2.678\n",
      "\n",
      "Learned weights:  [0. 1. 0.]\n",
      "Training MAE: 0.94\n",
      "Testing MAE: 2.49\n"
     ]
    }
   ],
   "source": [
    "def full_exercise(rf_max_depth=None,rf_n_estimators=100):\n",
    "    l1 = LinearRegression()\n",
    "    l2 = RandomForestRegressor(random_state=42,max_depth=rf_max_depth,n_estimators=rf_n_estimators)\n",
    "    l3 = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "    m1 = Pipeline(\n",
    "        steps=[\n",
    "            ('scaler',MaxAbsScaler()),\n",
    "            ('learner',l1)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    m2 = Pipeline(\n",
    "        steps=[\n",
    "            ('scaler',MaxAbsScaler()),\n",
    "            ('learner',l2)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    m3 = Pipeline(\n",
    "        steps=[\n",
    "            ('scaler',MaxAbsScaler()),\n",
    "            ('learner',l3)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size=.2,random_state=42)\n",
    "\n",
    "    m1.fit(Xtrain,ytrain)\n",
    "    m2.fit(Xtrain,ytrain)\n",
    "    m3.fit(Xtrain,ytrain)\n",
    "\n",
    "    p1 = m1.predict(Xtrain)\n",
    "    p2 = m2.predict(Xtrain)\n",
    "    p3 = m3.predict(Xtrain)\n",
    "\n",
    "    t1 = m1.predict(Xtest)\n",
    "    t2 = m2.predict(Xtest)\n",
    "    t3 = m3.predict(Xtest)\n",
    "\n",
    "    mae1 = mean_absolute_error(ytrain,p1)\n",
    "    mae2 = mean_absolute_error(ytrain,p2)\n",
    "    mae3 = mean_absolute_error(ytrain,p3)\n",
    "\n",
    "    tmae1 = mean_absolute_error(ytest,t1)\n",
    "    tmae2 = mean_absolute_error(ytest,t2)\n",
    "    tmae3 = mean_absolute_error(ytest,t3)\n",
    "\n",
    "    print('MAEs\\n')\n",
    "    print(\n",
    "        'Training:\\n',\n",
    "        '\\nLinear Regression:',round(mae1,3),\n",
    "        '\\nRF:',round(mae2,3),\n",
    "        '\\nKNN:',round(mae3,3),'\\n')\n",
    "    print(\n",
    "        'Testing:\\n',\n",
    "        '\\nLinear Regression:',round(tmae1,3),\n",
    "        '\\nRF:',round(tmae2,3),\n",
    "        '\\nKNN:',round(tmae3,3))\n",
    "    \n",
    "    X1 = pd.DataFrame({\n",
    "        'p1':p1,\n",
    "        'p2':p2,\n",
    "        'p3':p3\n",
    "    },index=Xtrain.index)\n",
    "    n_feats = X1.shape[1]\n",
    "\n",
    "    params = np.array([1/n_feats for n in range(n_feats)]) # Initialized with equal weights\n",
    "    params = np.clip(params,a_min=0,a_max=1) # Clipped parameter update, so parameters will be bounded from 0 to 1.\n",
    "    params = params/sum(params)\n",
    "    l = .001\n",
    "\n",
    "    for e in range(50):\n",
    "\n",
    "        yhat = X1@params\n",
    "\n",
    "        #sse = np.array((yhat - ytrain)**2).sum() # The SSE, but we don't need this for optimization.\n",
    "\n",
    "        grad = np.array(yhat - ytrain).T@(np.array((X1@params)*(-sum(params)**(-2)))[:, np.newaxis] + np.array(X1*(sum(params)**(-1)))) # The gradient for all parameters.\n",
    "\n",
    "        params = np.clip(params - l*grad,0,1) # Clipped parameter update, so parameters will be bounded from 0 to 1.\n",
    "\n",
    "        params = params/sum(params)\n",
    "        #print(sse)\n",
    "\n",
    "    yhat = X1@params/sum(params)\n",
    "\n",
    "    mae_ = round(mean_absolute_error(ytrain,yhat),3)\n",
    "\n",
    "    X1_test = pd.DataFrame({\n",
    "        'p1':t1,\n",
    "        'p2':t2,\n",
    "        'p3':t3\n",
    "    })\n",
    "\n",
    "    ypred = X1_test@(params/sum(params))\n",
    "\n",
    "    tmae_ = round(mean_absolute_error(ytest,ypred),3)\n",
    "\n",
    "    print(\"\\nLearned weights: \",np.round(params/sum(params),3))\n",
    "\n",
    "    print(\"Training MAE:\",mae_)\n",
    "    print(\"Testing MAE:\",tmae_)\n",
    "    \n",
    "    return\n",
    "\n",
    "full_exercise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed that is the observation, that the Random Forest is the only remaining estimator.\n",
    "\n",
    "Certainly, I expect this weighted-averaging technique to be a weaker final estimator than a simple regression with an intercept, but if SMEs have a strong desire to consider independent estimates directly in a weighted-averaging scheme, then estimating weights via optimization can yield a stronger estimator than an SME-selected weighting scheme."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
