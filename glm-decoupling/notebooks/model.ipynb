{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoupling Generalized Linear Model Distribution Choices from Canonical Link Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "This data will have an exponentially distributed while the log of the target can be expressed as a linear combination of predictors. In being exponentially distributed, the mean is strictly equal to the standard deviation. This is a different characteristic than seen in a Poisson-distributed target, where the mean is equal to the variance, while the canonical link function is a log-linear model. In fact, the canonical link function of an exponential-distribution response is the Negative Inverse:\n",
    "\n",
    "$ XB = - \\mu ^{-1}$\n",
    "\n",
    "Here, because the target's log can be linearly expressed, the mean will be expressed as such:\n",
    "\n",
    "$ XB = ln(\\mu) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Generate some example data\n",
    "np.random.seed(0)\n",
    "n_samples = 100\n",
    "X = np.random.rand(n_samples, 2)  # two predictors\n",
    "true_beta = np.array([1.0, -2.0])  # true coefficients\n",
    "true_intercept = 0.5\n",
    "\n",
    "# Generate Poisson-distributed response variable\n",
    "mu_ = np.exp(true_intercept + X @ true_beta)\n",
    "lambda_ = 1/mu_\n",
    "scale_ = 1/lambda_\n",
    "\n",
    "y = np.random.exponential(scale_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  1.2696557752766635 \n",
      "std:  1.5442965676889258 \n",
      "var:  2.384851888975797\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    'Mean: ',np.mean(y),\n",
    "    '\\nstd: ',np.std(y),\n",
    "    '\\nvar: ',np.var(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Poisson Optimization to an Exponential Distributed Target with the Log Link\n",
    "\n",
    "Through data analysis, if the log-linear model is desired due to strong log-linear correlations with predictors, or the advantagous quality of factor decomposition, then one should not simply apply Poisson regression due to the log-linear link function being canonical for Poisson regression without observing the mean and spread characteristics of the target, with respect to distribution parameter lambda. Lambda, the mean and the variance relate in the Poisson distribution through equality:\n",
    "\n",
    "$ \\lambda = \\mu = \\sigma ^ 2$\n",
    "\n",
    "If the relationship between the mean and variance shows that the target is Poisson distributed, then Poisson regression would be appropriate. In this exercise, the target belongs to the Exponential distribution, so the variance is not equal to the mean. Poisson regression optimization will estimate values for beta, but they may vary from the true parameter values more than desired.  \n",
    "\n",
    "Without derivation, the log-likelihood of the Poisson distribution is:  \n",
    "\n",
    "$ l = -ln(X!) + X ln(\\lambda) - \\lambda $  \n",
    "\n",
    "With mean estimate and data:\n",
    "\n",
    "$ \\mu = exp(XB); X = Y $  \n",
    "\n",
    "Because beta does not appear in the first term, the negative log of a factorial, this conveniently disappears when the derivative in terms of beta is taken. The log-likelihood function to maximize becomes:  \n",
    "\n",
    "$ l = YXB - exp(XB) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated intercept: 0.4804326098883521\n",
      "Estimated coefficients: [ 0.75846516 -1.54795698]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lambda_ = mu_\n",
    "#y = np.random.poisson(lambda_)\n",
    "\n",
    "# Define the negative log-likelihood for Poisson regression\n",
    "def poisson_neg_log_likelihood(params, X, y):\n",
    "    intercept = params[0]\n",
    "    beta = params[1:]\n",
    "    linear_prediction = intercept + X @ beta\n",
    "    mu_ = np.exp(linear_prediction)\n",
    "    lambda_ = mu_\n",
    "    neg_log_likelihood = -np.sum(y * linear_prediction - lambda_)\n",
    "    return neg_log_likelihood\n",
    "\n",
    "# Initial guess for the parameters\n",
    "initial_params = np.zeros(X.shape[1] + 1)\n",
    "\n",
    "# Perform the optimization\n",
    "result = minimize(poisson_neg_log_likelihood, initial_params, args=(X, y), method='L-BFGS-B')\n",
    "\n",
    "# Extract the estimated parameters\n",
    "estimated_intercept = result.x[0]\n",
    "estimated_beta = result.x[1:]\n",
    "\n",
    "print(f\"Estimated intercept: {estimated_intercept}\")\n",
    "print(f\"Estimated coefficients: {estimated_beta}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Exponential Optimization to an Exponential Distributed Target with the Log Link\n",
    "\n",
    "By analyzing the target, one would observe in this exercise that the mean is more nearly equal to the standard deviation than the variance. This would motivate the modeler to choose the Exponential distribution for optimizing the target through Maximum Likelihood Estimation, where the relationship between distribution parameter lambda, the eman and the variance is:\n",
    "\n",
    "$ \\lambda = \\frac{1}{\\mu} = \\frac{1}{\\sigma} \\rightarrow \\mu = \\sigma$  \n",
    "\n",
    "Without derivation, the log-likelihood of the Exponential distribution is:  \n",
    "\n",
    "$ l = ln(\\lambda) - \\lambda X $  \n",
    "\n",
    "With mean estimate and data:\n",
    "\n",
    "$ \\mu = exp(XB); X = Y $  \n",
    "\n",
    "The log-likelihood function to maximize becomes:  \n",
    "\n",
    "$ l = -XB - exp(-XB)Y $\n",
    "\n",
    "Exponential distribution MLE for solving the regression yields different coefficients than the Poisson optimization solution, as expected, and in this exercise, the intercept varies further from the true intercept, while the predictor coefficients are closer to their true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated intercept: 0.448432417057888\n",
      "Estimated coefficients: [ 0.87256177 -1.62201764]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lambda_ = 1/mu_\n",
    "scale_ = 1/lambda_\n",
    "\n",
    "#y = np.random.exponential(scale_)\n",
    "\n",
    "def exponential_neg_log_likelihood(params, X, y):\n",
    "    intercept = params[0]\n",
    "    beta = params[1:]\n",
    "    linear_prediction = intercept + X @ beta\n",
    "    mu_ = np.exp(linear_prediction)\n",
    "    lambda_ = 1/mu_\n",
    "    neg_log_likelihood = -np.sum(-linear_prediction - lambda_ * y)\n",
    "    return neg_log_likelihood\n",
    "\n",
    "\n",
    "# Initial guess for the parameters\n",
    "initial_params = np.zeros(X.shape[1] + 1)\n",
    "\n",
    "# Perform the optimization\n",
    "result = minimize(exponential_neg_log_likelihood, initial_params, args=(X, y), method='L-BFGS-B')\n",
    "\n",
    "# Extract the estimated parameters\n",
    "estimated_intercept = result.x[0]\n",
    "estimated_beta = result.x[1:]\n",
    "\n",
    "print(f\"Estimated intercept: {estimated_intercept}\")\n",
    "print(f\"Estimated coefficients: {estimated_beta}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Least-Squares (Gaussian assumptions) to an Exponential Distributed Target with the Log Link\n",
    "\n",
    "A very common technique in machine learning optimization is to minimize the Sum of Squared errors in order to solve for beta. The SSE cost function has the advantage (for the identity link - traditional linear regression) of an analytical solution. The relationship between the sum of squared errors and the Gaussian distribution is that Gaussian maximum likelihood estimation is a derivation of the sum of squared errors cost function - they are indelibly linked.\n",
    "\n",
    "$ L = (2 \\pi \\sigma ^ 2)^{\\frac{1}{2}} exp( -\\frac{1}{2 \\sigma ^2} (X - \\mu)^2 )$\n",
    "\n",
    "$ l = - \\frac{1}{2}ln(2 \\pi) - \\frac{1}{2}ln( \\sigma ^2 ) - \\frac{1}{2 \\sigma ^ 2}(X - \\mu)^2$\n",
    "\n",
    "with \n",
    "\n",
    "$ \\mu = exp(XB)$\n",
    "\n",
    "Optimizing beta to maximize the log-likelihood function is a matter of differentiating in terms of beta, which only appears in the third term. \n",
    "\n",
    "$ max_B (l) = max_B (- \\frac{1}{2 \\sigma ^ 2}(X - \\mu)^2) = max_B (- \\frac{1}{2}(X - \\mu)^2) $  \n",
    "\n",
    "This is the sum of squared errors cost function, derived from maximum likelihood estimation on the Gaussian distribution. \n",
    "\n",
    "With mean estimate and data:\n",
    "\n",
    "$ \\mu = exp(XB); X = Y $  \n",
    "\n",
    "The log-likelihood function to maximize becomes:  \n",
    "\n",
    "$ l = - \\frac{1}{2}ln( \\sigma ^2 ) - \\frac{1}{2 \\sigma ^ 2}(Y - exp(XB))^2 $\n",
    "\n",
    "If the target is Gaussian, this technique for optimization will be appropriate, but if the target is exponentially distributed, this technique will yield coefficients that are farther from true values. Here, optimization converges, but the predictor coefficients are farther from the true values than in either Poisson or Exponential regression. The intercept remains in the region of the true value. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated intercept: 0.5483205461701764\n",
      "Estimated coefficients: [ 0.60481991 -1.47466755]\n",
      "Estimated sigma: 1.4234723049616829\n"
     ]
    }
   ],
   "source": [
    "#Equivalent SSE function, not estimating sigma\n",
    "def sse_neg_log_likelihood(params, X, y):\n",
    "    intercept = params[0]\n",
    "    beta = params[1:-1]\n",
    "    linear_prediction = intercept + X @ beta\n",
    "    mu_ = np.exp(linear_prediction)\n",
    "    residuals = y-mu_\n",
    "    neg_log_likelihood = -np.sum(-.5 * ((residuals) ** 2))\n",
    "    return neg_log_likelihood\n",
    "\n",
    "# Define the negative log-likelihood for Gaussian regression\n",
    "def gaussian_neg_log_likelihood(params, X, y):\n",
    "    intercept = params[0]\n",
    "    beta = params[1:-1]\n",
    "    sigma=params[-1]\n",
    "    linear_prediction = intercept + X @ beta\n",
    "    mu_ = np.exp(linear_prediction)\n",
    "    residuals = y - mu_\n",
    "    neg_log_likelihood = np.sum(.5*np.log(sigma**2) + .5*(residuals**2) / (sigma**2))\n",
    "    return neg_log_likelihood\n",
    "\n",
    "\n",
    "# Initial guess for the parameters\n",
    "initial_params = np.concatenate([np.zeros(X.shape[1] + 1),np.ones(1)])\n",
    "\n",
    "# Perform the optimization\n",
    "result = minimize(gaussian_neg_log_likelihood, initial_params, args=(X, y), method='L-BFGS-B')\n",
    "\n",
    "# Extract the estimated parameters\n",
    "estimated_intercept = result.x[0]\n",
    "estimated_beta = result.x[1:-1]\n",
    "estimated_sigma = result.x[-1]\n",
    "\n",
    "print(f\"Estimated intercept: {estimated_intercept}\")\n",
    "print(f\"Estimated coefficients: {estimated_beta}\")\n",
    "print(f\"Estimated sigma: {estimated_sigma}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
